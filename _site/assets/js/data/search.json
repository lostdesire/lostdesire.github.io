[
  
  {
    "title": "Day29",
    "url": "/posts/Day29/",
    "categories": "BoostCamp, Week6",
    "tags": "dl, data, recsys",
    "date": "2023-12-14 20:00:00 +0900",
    





    
    "snippet": "Book-Recommendation 대회  대회 환경세팅  baseline 모델 돌려보기피어세션  ensemble이나 전처리에 대한 대화오피스아워  Book Recommendation 베이스라인 모델/코드 설명",
    "content": "Book-Recommendation 대회  대회 환경세팅  baseline 모델 돌려보기피어세션  ensemble이나 전처리에 대한 대화오피스아워  Book Recommendation 베이스라인 모델/코드 설명"
  },
  
  {
    "title": "Day28",
    "url": "/posts/Day28/",
    "categories": "BoostCamp, Week6",
    "tags": "dl, data, recsys",
    "date": "2023-12-13 20:00:00 +0900",
    





    
    "snippet": "Book-recommendation 대회  대회 환경 세팅피어세션  대회 환경 세팅마스터클래스  이현호 마스터님의 &lt;강의/프로젝트 설계 의도 설명 / 데이터 사이언티스트 준비물&gt;",
    "content": "Book-recommendation 대회  대회 환경 세팅피어세션  대회 환경 세팅마스터클래스  이현호 마스터님의 &lt;강의/프로젝트 설계 의도 설명 / 데이터 사이언티스트 준비물&gt;"
  },
  
  {
    "title": "Day27",
    "url": "/posts/Day27/",
    "categories": "BoostCamp, Week6",
    "tags": "dl, data, recsys",
    "date": "2023-12-12 20:00:00 +0900",
    





    
    "snippet": "Model Based CF      Unsupervised Learning Model                  Latent Factor Model                  유저와 아이템 사이 다양한 정보를 담은 데이터를 축약된 벡터 공간에 표현                      SVD                          차원 축...",
    "content": "Model Based CF      Unsupervised Learning Model                  Latent Factor Model                  유저와 아이템 사이 다양한 정보를 담은 데이터를 축약된 벡터 공간에 표현                      SVD                          차원 축소 용도로도 많이 활용되는 SVD를 이용              행렬을 분해 → 차원 축소 → 다시 복구              $R=U\\sum\\limits V^{T}\\qquad(Full SVD)$              $R\\approx\\hat{U}\\sum_{k}\\hat V^{T}=\\hat R \\qquad(Truncated SVD)$              한계점                                  User와 Item이 많아지면 예측력 감소                  결측치가 많은 데이터에 정상 작동 x                                          현실의 데이터는 결측치가 대부분                                                                                                                MF                          SVD는 $R=U\\sum\\limits V^{T}$로 3가지 행렬의 곱으로 표현              MF는 $R\\approx PQ^T$로 User Latent Factor와 Item Latent Factor 2가지 행렬의 곱으로 표현              목적함수                                  $\\underset{P,Q}{min} \\underset{observed\\;r_{u,i}}{\\sum\\limits}(r_{u,i}-p_{u}q_{i}^{T})^2$                                            P,Q를 구하기 위해 SGD 활용              과적합 방지를 위한 L2 정규화 term                                  $\\underset{P,Q}{min} \\underset{observed\\;r_{u,i}}{\\sum\\limits}(r_{u,i}-p_{u}q_{i}^{T})^2+\\lambda(\\lVert p_{u}\\rVert ^{2}+\\lVert q_{i}\\rVert ^2)$                                            기존 편향된 정도를 반영하기 위해 bias term 추가                                  $\\underset{P,Q}{min} \\underset{observed\\;r_{u,i}}{\\sum\\limits}(r_{u,i}-\\mu-b_{u}-b_{i}-p_{u}q_{i}^{T})^2+\\lambda(\\lVert p_{u}\\rVert ^{2}+\\lVert q_{i}\\rVert ^2)$                                            SGD를 이용한 MF의 한계점                                  p와 q를 변수를 가지므로 빠른 계산 불가능                  SGD를 학습하는 과정에서 업데이트를 여러번 → 해당 단점 학습할 때마다 누적                  u, i가 커질수록 반복문으로 인하여 연산량 증가                                                                          ALS                          q를 고정값으로 두고 p만 변수로 (p를 고정값으로 두고 q만 변수로)              $p_{u}=(Q^{T}Q+\\lambda I)^{-1}Q^{T}r_{u}$              $q_{i}=(P^{T}P+\\lambda I)^{-1}P^{T}r_{i}$              여러 연산이 필요하지 않아 계산 속도 증가              element 단위가 아닌 행렬 단위 연산이 가능 → $u \\times i$ 만큼 반복문을 돌아야하는 문제 해결                                                Hybrid Recommender System          여러가지 추천 방법을 사용      하나의 접근법을 다른 접근법에 더하거나 하나의 model로 만드는 등의 방법이 존재      cold start, sparsity 같은 문제들을 효과적으로 극복 가능                  CB + CF          Memory Based CF + Model Based CF          Clustering - Memory Based CF 등                            Hybridization 기술                              Weighted                          서로 다른 추천의 점수를 가중 평균                                            Switching                          추천 시스템을 변경하며 추천                                            Mixed                          서로 다른 추천에서 나오는 추천을 섞어서 추천                                            Feature Combination                          다른 추천에서 나온 특징들을 합쳐 하나의 추천 알고리즘에서 사용                                            Feature Augmentation                                          Feature Combination                을 여러번 연결                                                          Cascade                          상위 추천 → 하위 추천                                            Meta-level                          추천 시스템 model이 입력으로 사용되는 model로 추천              model 자체를 입력으로 전달                                                Supervised Learning Model                  Linear(Logistic) Regression                    Random Forest                    Naive Bayes                  일반 Bayes 공식                          $P(Y=k\\mid X)=\\frac{P(X\\mid Y=k)P(Y=k)}{P(X)}$                                다변량 Bayes 공식                          $P(Y=k\\mid X_{1},X_{2},…,X_{n})=\\frac{P(X_{1}\\mid Y=k)P(X_{2}\\mid Y=k)…P(X_{n}\\mid Y=k)P(Y=k)}{P(X_{1},X_{2},…,X_{n})}$                                X 변수가 서로에 대해 독립적이라는 가정  -&gt; $P(X_{1}\\mid Y,X_{2},X_{3})P(X_{1}\\mid Y,X_{3},X_{8},X_{10})$ 같은 조건부 수식 제거 가능          binary 변수 많을 때 (one-hot encoding) BernoulliNB 사용하면 좋음                            GBDT (Gradient Boosting Decision Tree)                  NB에서 독립이라고 가정했던 다른 변수와의 상호 작용을 반영                      XGBoost                          Parallelization                                  기존 GBDT의 순차적 업데이트 개선                                            Level-wise                                  depth-fist                                            Regularization                                  overfitting 개선                                                                          LightGBM                          Leaf-wise                                  information gain이 있는 곳으로만 분할                  level-wise 보다 좀 더 빠름                  과적합에는 취약                                            GOSS (Gradient-based One-Side Sampling)                                  Top N개 데이터 인스턴스 + 랜덤 샘플링 데이터 인스턴스만 활용                  undersampling 되어 데이터 인스턴스 수 감소                                            EFB (Exclusive Feature Bundling)                                  sparse한 feature가 많을 때 번들링 하는 EFB                  여러 feature를 하나의 feature인 것처럼 줄여 feature 수 감소                                            O(data * feature) → O(data_GOSS * Bundles)                                            CatBoost                          Level-wise                                  overfitting 방지                                            Ordered Boosting                                  n+1번째 order의 잔차를 계산할 때 n번째까지 데이터로 학습한 모델에 $x_{n+1}$을 넣어 $y_{n+1}$과 비교                                            Ordered TS(Target Statics)                                  n+1번째 order의 TS를 계산할 때 n번째까지 데이터의 statistics 정보를 이용                  y값의 평균을 사용                                            예측 시점에서 사용할 수 없는 데이터가 데이터 셋에 포함되지 않도록                                          피어세션  Context-aware Recommendation 리뷰부캠살롱  E-commerce  Data AnalisysGit 특강  Branch  Commit  Issue  Pull Request  Tag"
  },
  
  {
    "title": "Day26",
    "url": "/posts/Day26/",
    "categories": "BoostCamp, Week6",
    "tags": "dl, data, recsys",
    "date": "2023-12-11 20:00:00 +0900",
    





    
    "snippet": "추천 시스템 정의  정보 필터링 기술의 일종  아마존의 1/3, 넷플릭스의 3/4가 추천시스템을 통해 컨텐츠 소비      Push          사용자가 관심을 가질 만한 정보를 시스템이 밀어내듯이 제공            Pull          사용자의 의도가 명확할 때 사용자의 의도에 맞는 항목을 찾고 추천        대부분의 추천 시스템은...",
    "content": "추천 시스템 정의  정보 필터링 기술의 일종  아마존의 1/3, 넷플릭스의 3/4가 추천시스템을 통해 컨텐츠 소비      Push          사용자가 관심을 가질 만한 정보를 시스템이 밀어내듯이 제공            Pull          사용자의 의도가 명확할 때 사용자의 의도에 맞는 항목을 찾고 추천        대부분의 추천 시스템은 Push와 Pull의 중간에 위치추천 시스템 개발 시 고려할 점1. 데이터  사용자 / 아이템 관련 정보  상호작용 했을 때 발생한 피드백 (CTR, 평점 등)2. Task 설정  단기 목표          클릭 수, 특정 기간의 총 수익        장기 목표          사이트 소비 시간 증가, 재방문 및 사용자 유지 비율 개선, 구독자 증가        일반적으로 최적화 대상은 1가지 (2개 이상을 대상으로 하는 경우도 존재)3. 목적함수 설계  설정된 Task에 맞는 목적 함수 설계  미래 특정 상황에서 사용자에게 아이템을 제안했을 때 발생하는 가치를 예측4. 랭킹  목적 함수의 기댓값을 최대화할 수 있도록 랭킹 선정 방법 정의  연관성, 여러 지표, 비즈니스 규칙 등의 요소를 적절히 조합추천 시스템 성능 평가 방법      오프라인 평가          수집된 데이터를 train, valid, test 3가지로 분할하여 성능 평가 시 사용              데이터 분할 방법                              Leave One Last                          마지막 구매를 test              마지막에서 2번째를 valid              장점                                  학습 시 많은 데이터 사용 가능                                            단점                                  테스트 성능이 전체적인 성능을 반영한다고 보기 어려움                  모델이 테스트 데이터 상호작용을 특징으로 학습할 가능성 있음                  과거의 데이터가 미래의 정보를 알게되는 Data Leakage가 발생                                                                          Temporal User/Global Split                          특정 시점을 이용한 분할 전략              Temporal User                                  시간 순서에 따라 일정 비율로 데이터 분할                  Leave One Last와 유사                  Data Leakage 발생                                            Temporal Global                                  유저들의 분할 시점을 고정                  학습 및 검증에 사용할 수 있는 상호작용이 적을 수 있음                                                                          Random Split                          시간 순서에 관계 없이 Random하게 분할              사용하기 쉬움              많은 training data              Data Leakage 발생                                            User Split                          사용자가 겹치지 않게 사용자를 기준으로 분할              Cold-Start 대응 가능              User-free 모델에서만 사용 가능              Data Leakage 발생                                            K-Fold Cross Validation                          training data를 k개의 fold로 분할하여 한 fold씩 valid data로 사용                                            Time Series Cross Validation                          일반적인 CV은 미래를 학습하여 과거를 예측하는 문제가 있음              fold 분할 시 시간을 고려 (미래의 데이터 학습 x)              데이터를 늘려가는 방식              데이터를 줄여가는 방식                                                  예측 알고리즘 평가 지표                              평점 예측 문제                          RMSE                                  $RMSE = \\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{N}(y_{i}-\\hat{y_{i}})^2}$                                            MAE                                  $MAE = \\frac{1}{N}\\sum\\limits_{i=1}^{N} \\left\\vert y_{i}-\\hat{y_{i}}\\right\\vert$                                                                          랭킹 문제                          Precision@K                                  추천한 K개의 아이템 중 유저가 관심있는 아이템의 비율                                            Recall@K                                  유저가 관심있는 아이템 중 추천한 아이템의 비율                                                            AP@K                                  Precision@1 ~ Precision@K 평균                  $AP@K = \\frac{1}{m}\\sum\\limits_{i=1}^{K}Precision@i \\cdot rel(i)$                  $m$ : 사용자가 반응한 아이템의 갯수                  $rel(i)$ : item i와의 relevance score                                                            MAP@K                                  $MAP@K=\\frac{1}{\\left\\vert U\\right\\vert}\\sum\\limits_{u=1}^{\\left\\vert U\\right\\vert}(AP@K)_{u}$                  $\\left\\vert U\\right\\vert$ : 유저 수                                                            Relevance Score                                  CG(Cumulative Gain)                                          $CG_{K}=\\sum\\limits_{i=1}^{K}rel_{i}$                                                        DCG(Discounted CG)                                          $DCG_{K}=\\sum\\limits_{i=1}^{K}\\frac{rel_{i}}{log_2(i+1)}$                                                        IDCG(Ideal DCG)                                          $IDCG_{K}=\\sum\\limits_{i=1}^{K}\\frac{rel_{i}^{OPT}}{log_{2}(i+1)}$                                                                            NDCG(Normalized DCG)                                          $NDCG_{K}=\\frac{DCG}{IDCG}$                                                        Hit rate                                                                                오프라인 평가 지표의 문제점                  정확성 개선이 실제 시스템 성능 향상이라고 연관 짓기 어려움          서비스 만족도를 높인다고 보기 어려움                            특성 평가 지표                  Coverage                          전체 유저/아이템 중 추천 시스템이 추천하는 유저/아이템의 비율                                Novelty                          새로운 아이템 추천              추천하는 인기 항복 수를 줄이기 위해 사용                                Personalization                          개인화된 추천              다른 사용자와 추천된 아이템을 비교                                Serendipity                          의외의 아이템 추천              online 평가에서 주로 사용                                Diversity                          얼마나 다양한 아이템이 추천되는지              도메인에 따라 여러 방법으로 측정 가능                                  카테고리나 장르가 몇가지 추천되는지                  판매자, 작가 등이 몇가지 추천되는지                  가격 분포의 첨도                  추천된 아이템을 임베딩하여 유사도 측정                                                                              온라인 평가          이상적인 테스트                  동일 유저에게 A안을 제안하고 시간을 돌려 B안을 제안하는 것          현실에서는 A안과 B안 둘 중 하나만 겪을 수 있음                    현실적인 테스트                  유저를 두 그룹으로 분할 → 각각 A안과 B안을 보여준 후 평균 비교          두 그룹의 특성이 모든 면에서 비슷해야함          그룹이 나뉘는 시점도 비슷해야함                    Content Based Filtering      아이템 특성화          Original data → One-hot encoding → Embedding      추천 시스템의 경우 아이템의 특성이 왜곡될 수 있어 label encoding은 잘 안함              TF-IDF                  다른 문서에는 많지 않고, 해당 문서에만 자주 등장하는 단어에 높은 값          $TF-IDF = freq_{w,d}\\times log\\frac{N}{n_{W}}$                            Word2Vec                  유사도          특성화된 아이템이 서로 얼마나 비슷한지              Jaccard                  0, 1로 이루어진 binary 데이터로 변환 후, 두 벡터의 교집합과 합집합을 구함          $J(A,B)=\\frac{\\left\\vert A\\cap B\\right\\vert}{\\left\\vert A\\cup B\\right\\vert} = \\frac{\\left\\vert A\\cap B\\right\\vert}{\\left\\vert A\\right\\vert+\\left\\vert B\\right\\vert-\\left\\vert A\\cup B\\right\\vert}$          공식이 간단하여 계산 속도가 빠름                            Euclidean                  n차원으로 주어진 아이템 속성 A,B 벡터 사이의 최단 거리          $L_{2}=\\sqrt{(A_{1}-B_{1})^2+(A_{2}-B_{2})^2+\\cdots+(A_{n}-B_{n})^2}=\\sqrt{\\sum\\limits_{i=1}^{n}(A_{i}-B_{i})^2}$                            Cosine                  각도 기반          $Similarity=cos(\\theta)=\\frac{A\\cdot B}{|A|\\cdot|B|}=\\frac{\\sum\\limits_{i=1}{n}A_{i}B_{i}}{\\sqrt{\\sum\\limits_{i=1}^{n}(A_{i})^2}\\times\\sqrt{\\sum\\limits_{i=1}^{n}(B_{i})^2}}$          -1 ~ 1 사이의 값                            Pearson correlation coefficient                  $Pearson_sim=\\rho_{A,B}=\\frac{\\sum\\limits_{i=1}^{n}(A_{i}-\\bar{A})(B_{i}-\\bar{B})}{\\sqrt{\\sum\\limits_{i=1}^{n}(A_{i}-\\bar{A})^2}\\sqrt{\\sum\\limits{\\sum\\limits_{i=1}^{n}(B_{i}-\\bar{B})^2}}}$          -1 ~ 1 사이의 값          선형 상관 관계만 나타내므로 0이 나오면 data의 관계를 알 수 없음                    Memory Based Collaborative Filtering  내가 좋아하는 장르, 작가, 출판사의 책  -&gt; Content Based Filtering  나와 비슷한 성향의 사람들이 읽은 책  -&gt; Collaborative Filtering      Memory Based CF          유저와 아이템의 상호 작용      최적화나 훈련 과정 필요 없음      접근 방식이 쉬움      sparse한 데이터의 경우 성능 저하      데이터가 많아지면 계산량의 증가로 인한 확장성의 한계              User Based CF                  새로운 사용자의 경우 정보가 없어 추천이 어려움 (Cold-Start)                          평점을 기반으로 User-Item matrix 생성              빈 값을 0으로 채우고, 사용자 사이의 유사도 계산              유사도를 통한 가중 평균으로 평점 예상                                                  Item Based CF                  시간에 따라 유사도 변화가 적고 User based CF에 비해 계산량이 적음          사용자가 아이템에 feedback한 정보가 많아야 함                          평점을 기준으로 User-Item matrix 생성              빈 값을 0으로 채우고, 아이템 사이의 유사도 계산              유사도를 통한 가중 평균으로 평점 예상                                                Model Based CF          Unsupervised Learning                  Clustering          Latent Factor                    Supervised Learning                  ML          DL                    데이터 패턴을 학습하여 추천 가능      유저-아이템 관계의 잠재적 특성 및 패턴 찾기 가능      유저, 아이템 개수가 늘어나도 좋은 성능      학습 이후 서빙 속도 빠름              Clustering                  clustering을 다른 추천 방법론과 함께 사용하여 효과적인 추천 수행 가능                          군집내 다른 사용자의 선호 아이템 추천              clustering 후 CF를 통해 예측 정확도 향상              비슷한 유저 군집 데이터를 추출하여 아이템 선호도를 계산하고 사전 확률로 활용하여 베이지안 방법론 적용 가능                                분할 된 데이터의 희소성 문제로 실제 데이터에 적용시 성능 저하          군집 개수 등 파라미터를 직접 설정해야함                      K-Means Clustering                          군집을 k개로 나누는 알고리즘              구형 분포가 아닌 경우 군집화 성능이 저하              아웃라이어에 민감                                  랜덤하게 초기 중심점 k개 배치                  각 데이터를 가장 가까운 중심점으로 할당                  모인 데이터 군집의 평균으로 중심점 업데이트                  중심점이 업데이트 되지 않을 때까지 2~3 반복                                                                          DBSCAN                          차원 수가 커질수록 데이터 밀도가 떨어져서 군집 형성이 어려움 (차원의 저주)                                            GMM(Gaussian Mixture Model)                          데이터가 정규성을 만족하지 않는 경우 성능 저하              계산량 많음                                            OPTICS                                Hierarchical clustering                          모든 경우에 대해 반복적으로 유사도를 계산              계산량 많음                                          피어세션  DL RecSys(2) 리뷰"
  },
  
  {
    "title": "Day25",
    "url": "/posts/Day25/",
    "categories": "BoostCamp, Week5",
    "tags": "dl, data, recsys",
    "date": "2023-12-08 20:00:00 +0900",
    





    
    "snippet": "심화과제 2스페셜 피어세션피어세션  DL RecSys (1) 리뷰주간 회고  이번 주는 처음보는 내용들이 대부분이고 내용도 어려운 편이라 강의 수는 적어도 복습에 시간을 많이 쓰게 된 것 같다.  실습과 과제를 하면서 이해가 안됐던 부분들이 머리 속으로 조금씩 들어오는 느낌",
    "content": "심화과제 2스페셜 피어세션피어세션  DL RecSys (1) 리뷰주간 회고  이번 주는 처음보는 내용들이 대부분이고 내용도 어려운 편이라 강의 수는 적어도 복습에 시간을 많이 쓰게 된 것 같다.  실습과 과제를 하면서 이해가 안됐던 부분들이 머리 속으로 조금씩 들어오는 느낌"
  },
  
  {
    "title": "Mathjax 적용하기",
    "url": "/posts/Mathjax-%EC%A0%81%EC%9A%A9/",
    "categories": "Github, blog",
    "tags": "mathjax",
    "date": "2023-12-07 20:00:00 +0900",
    





    
    "snippet": "_config.yml 수정 markdown: kramdown highlighter: rouge lsi: false excerpt_separator: \"\\n\\n\" incremental: falsemathjax_support.html 추가  _includes 폴더에 mathjax_support.html 추가    &lt;script type=\"text/x...",
    "content": "_config.yml 수정 markdown: kramdown highlighter: rouge lsi: false excerpt_separator: \"\\n\\n\" incremental: falsemathjax_support.html 추가  _includes 폴더에 mathjax_support.html 추가    &lt;script type=\"text/x-mathjax-config\"&gt;MathJax.Hub.Config({  TeX: {  \tequationNumbers: {      \tautoNumber: \"AMS\"  \t}  },  tex2jax: {  \tinlineMath: [ ['$', '$'] ],  \tdisplayMath: [ ['$$', '$$'] ],  \tprocessEscapes: true,  }});MathJax.Hub.Register.MessageHook(\"Math Processing Error\",function (message) {  alert(\"Math Processing Error: \"+message[1]);});MathJax.Hub.Register.MessageHook(\"TeX Jax - parse error\",function (message) {  alert(\"Math Processing Error: \"+message[1]);});&lt;/script&gt;&lt;script type=\"text/javascript\" asyncsrc=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\"&gt;&lt;/script&gt;        만약 작동하지 않는다면 다음과 같이 추가    ​&lt;script type=\"text/x-mathjax-config\"&gt;MathJax.Hub.Config({  TeX: {    equationNumbers: {      autoNumber: \"AMS\"    }  },  tex2jax: {  inlineMath: [ ['$', '$'], ['\\\\(', '\\\\)'] ],  displayMath: [ ['$$', '$$'], ['\\\\[', '\\\\]'] ],  processEscapes: true,}});MathJax.Hub.Register.MessageHook(\"Math Processing Error\",function (message) {    alert(\"Math Processing Error: \"+message[1]);  });MathJax.Hub.Register.MessageHook(\"TeX Jax - parse error\",function (message) {    alert(\"Math Processing Error: \"+message[1]);  });&lt;/script&gt;&lt;script type=\"text/javascript\" asyncsrc=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\"&gt;&lt;/script&gt;      default.html 추가하기  _layouts 폴더에 있는 default.html에 해당 코드 추가{% if page.use_math %}{% include mathjax_support.html %}{% endif %}mathjax post에 적용하기  적용할 post의 front-matter에 use_math 항목 추가    ---title: Mathjax 적용하기date: 2023-12-07 20:00:00 +09:00categories: [Github, blog]tags: [mathjax]     # TAG names should always be lowercasetoc: truetoc_sticky: trueuse_math: true---      "
  },
  
  {
    "title": "Day24",
    "url": "/posts/Day24/",
    "categories": "BoostCamp, Week5",
    "tags": "dl, data, recsys",
    "date": "2023-12-07 20:00:00 +0900",
    





    
    "snippet": "MAB(Multi-Armed Bandit)  MAB의 정책          모든 슬롯머신을 동일한 횟수로 당김                  높은 reward 불가능                    일정 횟수 후 제일 높은 확률의 슬롯머신만 당김                  동일한 슬롯만 계속 당김                      Explor...",
    "content": "MAB(Multi-Armed Bandit)  MAB의 정책          모든 슬롯머신을 동일한 횟수로 당김                  높은 reward 불가능                    일정 횟수 후 제일 높은 확률의 슬롯머신만 당김                  동일한 슬롯만 계속 당김                      Exploration          더 많은 정보를 얻기 위해 새로운 arm을 선택하는 것        Exploitation          경험 / 관측을 토대로 가장 좋은 arm을 선택하는 것        \\[q_{*}(a) \\doteq \\mathbb{E}[R_{t} | A_{t} = a]\\]          $A_{t}$ : action      $R_{t}$ : reward      $q_{*}(a)$ : 액션 a에 따른 reward의 실제 기대값        $q_{*}(a)$에 대한 시간 t에서의 추정치 $Q_{t}(a)$를 최대한 정밀하게 구하는 것이 목표  greedy action 선택 → exploitation  다른 action 선택 → exploration      Greedy Algorithm          Simple Average Method      \\[Q_{t}(a) \\doteq \\frac{sum\\ of\\ rewards\\ when\\ a\\ taken\\ prior\\ to\\ t}{number\\ of\\ times\\ a\\ taken\\ prior\\ to\\ t} = \\frac{\\sum_{i=1}^{t-1}R_{i}\\cdot 1_{A_{i}=a}}{\\sum_{i=1}^{t-1}1_{A_{i}=a}}\\]            실제 기대값 $q_{*}(a)$의 가장 간단한 추정 방식으로 표본 평균을 사용      평균 리워드가 최대인 action을 선택하는 것 → Greedy                  처음 선택되는 action과 reward에 크게 영향을 받음(exploration이 부족)                          Epsilon-Greedy Algorithm          exploration이 부족한 greedy algorithm을 수정      ​일정한 확률에 의해 랜덤으로 슬롯머신을 선택      \\[A_{t} \\doteq \\underset{a}{argmax}\\begin{bmatrix}Q_{t}(a) + c\\sqrt\\frac{\\ln t}{N_{t}(a)}\\end{bmatrix}\\]                  $Q_{t}(a)$ : action a에 대한 reward의 추정치 (simple average)          $N_{t}(a)$ : action a를 선택한 횟수          c : exploration을 조정하는 하이퍼파라미터                      ​개별 아이템 ← 개별 action  ​추천 방식 ← MAB policy  ​클릭 여부 ← reward      유저 추천          ​클러스터링을 통해 비슷한 유저끼리 그룹화      필요 Bandit 개수 = 유저 클러스터 개수 x 후보 아이템 개수            유사 아이템 추천          주어진 아이템과 유사한 후보 아이템 리스트를 생성      필요 Bandit 개수 = 아이템 개수 x 후보 아이템 개수            Thompson Sampling          주어진 k개의 action에 해당하는 확률분포를 구하는 문제      베타 분포                  두 개의 양의 변수로 표현 가능한 확률 분포          \\[Beta(x|\\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\]                    $B(\\alpha, \\beta)$는 $\\alpha, \\beta$에 의해 정해지는 베타 함수          배너를 보고 클릭한 횟수 $\\alpha$          배너를 보고 클릭하지 않은 횟수 $\\beta$          배너를 클릭할 확률 ~ $Beta(\\alpha+1, \\beta+1)$          초기에는 랜덤 노출 → 베타 분포에서 샘플링하여 노출                          LinUCB          Contextual Bandit      유저 context 정보에 따라 동일한 action이더라도 다른 reward를 가짐 (개인화 추천)      \\[A_{t} \\doteq \\underset{a}{argmax}\\begin{bmatrix}x_{t,a}^{T}\\theta_{a}^{*} + \\alpha\\sqrt{x_{t,a}^{T}A_{a}^{-1}X_{t,a}}\\end{bmatrix} \\qquad where A_{a} = D_{a}^{T}D_{a}+I_{d}\\]                  $x_{t,a}$ : d-차원 컨텍스트 벡터          $\\theta_{a}^{*}$ : action a에 대한 d-차원 학습 파라미터          $D_{a}$ : m개의 컨텍스트 벡터로 구성된 $m\\times d$ 행렬                    피어세션  Item2Vec &amp; ANN 리뷰마스터클래스  이준원 마스터님의 &lt;ML Career 만들기 (Feat. AD-Tech)&gt;  이직 : 필수 조건 / 꼭 만족하지 않아도 되는 조건 고려  중요한 것 : 1. 개발 능력, 2. ML/DL 지식 및 이해도, 3) 커뮤니케이션 → 문제해결력  주니어 때는 하나의 도메인과 직무에서 깊이를 쌓는 것이 좋다."
  },
  
  {
    "title": "Day23",
    "url": "/posts/Day23/",
    "categories": "BoostCamp, Week5",
    "tags": "dl, data, recsys, wide&deep, deepfm, din, bst",
    "date": "2023-12-06 20:00:00 +0900",
    





    
    "snippet": "Wide &amp; Deep      Memorization          같이 자주 등장하는 item / feature를 과거로부터 학습      LR            Generalization          드물거나 전혀 발생한 적 없는 item / feature를 기존 관계로부터 발견      FM, DNN        두 특성을 가지는 ...",
    "content": "Wide &amp; Deep      Memorization          같이 자주 등장하는 item / feature를 과거로부터 학습      LR            Generalization          드물거나 전혀 발생한 적 없는 item / feature를 기존 관계로부터 발견      FM, DNN        두 특성을 가지는 구조를 결합      The Wide Component          Generalized Linear Model                  \\[y = w^Tx + b\\]                              Cross-Product Transformation                  \\[\\phi_{k}(x) = \\prod_{i=1}^{d}x_{i}^{c_{ki}} \\qquad c_{ki} \\in \\{0, 1\\}\\]                                    The Deep Component          Feed-Forward Neural Network                  3 layer (ReLU)          연속형 변수 → 그대로 사용          범주형 변수 → feature embedding 후 사용                      \\[P(Y = 1|x) = \\sigma(W_{wide}^{T}[x, \\phi(x)] + W_{deep}^{T}a^{(lf)} + b)\\]    Wide model은 offline / Deep model은 online에서 좋은 현상을 보이므로 두 모델을 결합하여 모두 좋은 성능DeepFM  Wide &amp; Deep model의 wide component는 feature engineering(Cross-Product Transformation)이 필요하다는 단점이 있음  wide component로 FM을 사용하여 deep component의 dense embedding 입력값을 공유  DeepFM = FM + DNN  \\(\\hat y = sigmoid(y_{FM}+y_{DNN})\\)DIN(Deep Interest Network)  다양한 관심사를 반영하기 위해 나온 모델          Embedding Layer              Local Activation Layer                  후보군이 되는 광고를 기존에 본 광고들의 연관성을 계산하여 가중치로 표현                    Fully-connected Layer      BST(Behavior Sequence Transformer)  CTR 예측과 NLP 번역 데이터 간의 공통점                  대부분 sparse feature            low/high-order feature interaction 모두 존재 (비선형적 관계)        Transformer의 encoder만 사용  vs DIN          local activation layer → transformer layer      user behavior feature → user behavior sequence        vs Transformer                  dropout, leakyReLU        추가                    layer 1~4개만        사용 (best는 1개)                    Custom Positional Encoding                  \\(pos(v_{i}) = t(v_{t}) - t(v_{i})\\)                    피어세션  CF 리뷰"
  },
  
  {
    "title": "Macbook Setting",
    "url": "/posts/Macbook_Setting/",
    "categories": "Mac, Setting",
    "tags": "mac",
    "date": "2023-12-05 20:00:00 +0900",
    





    
    "snippet": "언젠가 맥북을 다시 설정하는 날이 있을 때 한번에 해결하기 위한 포스트Karabiner  마우스를 사용할 시 인터넷 환경에서 뒤로가기 앞으로가기를 활성화 시키기          Karabiner를 설치 후 여러 권한들 허용      Karabiner-Elements → Complex Modifications → Add rule      하단의 Impo...",
    "content": "언젠가 맥북을 다시 설정하는 날이 있을 때 한번에 해결하기 위한 포스트Karabiner  마우스를 사용할 시 인터넷 환경에서 뒤로가기 앞으로가기를 활성화 시키기          Karabiner를 설치 후 여러 권한들 허용      Karabiner-Elements → Complex Modifications → Add rule      하단의 Import → Change mouse buttons (rev 2) Import      Change button4,5 to back,forward (rev 1) Enable      Devices → Mouse의 Modifiy events 활성화      Homebrew  터미널에서 homebrew 설치하기    /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"        버전 확인    brew --version        오류 발생시      $ brew --version  zsh: command not found: brew  # zshrc에 homebrew path 추가  $ echo 'export PATH=/opt/homebrew/bin:$PATH' &gt;&gt; ~/.zshrc  # zshrc 반영  $ source ~/.zshrc      Iterm2brew install --cask iterm2  Appearance → Theme - Minimal  Profiles → Session → Status bar enabled (세부설정으로 여러가지 설정가능)  Profiles → Colors → Color Presets로 다운 받은 iterm2 color 적용  iCloud 접근          /Users/UserName/Library/Mobile\\ Documents/com~apple~CloudDocs      Oh-my-zshsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"  p10k    git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k        테마 적용          vi ~/.zshrc → ZSH_THEME=”ZSH_THEME=”powerlevel10k/powerlevel10k”      재실행      Meslo Nerd Font 설치 후 재실행      아이콘이 다 보이면 y      세부 테마 설정      3 → 1 → 1 → 3 → 1 → 1 → 1 → 1 → 2 → 1 → n → 1 → y        추천 플러그인batbrew install batfdbrew install fdauto suggestionsgit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestionssyntax highlightinggit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting$vi ~/.zshrcsource ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting/zsh-syntax-highlighting.zshautojumpbrew install autojumpghbrew install ghVSCode  homebrew를 통해 vscode 설치하기    brew install --cask visual-studio-code        –cask는 GUI 응용프로그램 설치할 때 (~/Applications에 설치됨)  깔면 좋은 익스텐션          Korean Language Pack for Visual Studio Code (대부분 기본 설치)      Git Graph      Jupyter / Jupyter PowerToys      Material Icon Theme      Prettier        terminal font family          MesloLGS NF      anaconda3  homebrew를 통해 anaconda 설치하기    brew install --cask anaconda        버전 확인    conda -V        오류 발생시    $ echo 'export PATH=/opt/homebrew/anaconda3/bin:$PATH' &gt;&gt; ~/.zshrc$ source ~/.zshrc      Ruby &amp; Jekyll  github blog 로컬 테스트를 위한 jekyll 설치      brew install rbenv ruby-build  rbenv install -l  rbenv install 3.2.2 (2023-12-06 기준)  $ vi ~/.zshrc  [[ -d ~/.rbenv  ]] &amp;&amp; \\\texport PATH=${HOME}/.rbenv/bin:${PATH} &amp;&amp; \\\teval \"$(rbenv init -)\"  rbenv global 3.2.2  bundle install  gem install jekyll bundler      "
  },
  
  {
    "title": "Day22",
    "url": "/posts/Day22/",
    "categories": "BoostCamp, Week5",
    "tags": "dl, data, recsys, git",
    "date": "2023-12-05 20:00:00 +0900",
    





    
    "snippet": "생활코딩 egoing님 2차 특강      1차 특강 복습          git add      git commit                  –amend -m “example”          head가 가리키는 commit message 바뀜 (commit id도 달라짐)                    git checkout      gi...",
    "content": "생활코딩 egoing님 2차 특강      1차 특강 복습          git add      git commit                  –amend -m “example”          head가 가리키는 commit message 바뀜 (commit id도 달라짐)                    git checkout      git branch      git reset (–hard)                  checkout은 head를 reset은 branch를 변경                    git merge                  rebase, revert, cherry-pick은 3-way merge에 의해 conflict 발생                          git rebase          base를 변경하여 history가 마치 한 줄인 것처럼      이미 공개 저장소에 Push 한 커밋을 Rebase 하지 마라      타임라인의 축소 branch의 commit 순서 합치고 main의 commit 순서 합치고      git rebase main exp                  main의 base를 exp로 간주          main → exp                          main의 원래 base에서 첫 commit까지의 변화를 exp에 반영              main의 이후 commit을 새로 생긴 것과 결합                                exp1 → exp2 → exp2 &amp; main1 &amp; base (main1) → rebase1 &amp; main2 &amp; main1 (main2)                          git cherry-pick          다른 branch의 특정 commit 하나의 변경사항을 가져오는 것      main이 exp1에서 exp2의 변경사항을 가져오는 것      exp2 &amp; main2 &amp; exp1      git cherry-pick –abort                  cherry-pick을 되돌리고 싶으면 –abort를 사용                          git pull          git에 있는 remote repository 변경사항 local에 반영      fetch + merge      내용이 다를시 merge commit 자동 생성            git clone          remote repository 복사      init + remote add REPO_URL + pull      "
  },
  
  {
    "title": "Day21",
    "url": "/posts/Day21/",
    "categories": "BoostCamp, Week5",
    "tags": "dl, data, recsys, fm, ffm, gbm",
    "date": "2023-12-04 20:00:00 +0900",
    





    
    "snippet": "DL RecSys  GNN          Relation, Interaction에 적합      Non-Euclidean Space의 표현, 학습 가능 (sns, 분자)        NGCF          유저와 아이템 임베딩      상호작용 모델링                                            임베딩 레이어    ...",
    "content": "DL RecSys  GNN          Relation, Interaction에 적합      Non-Euclidean Space의 표현, 학습 가능 (sns, 분자)        NGCF          유저와 아이템 임베딩      상호작용 모델링                                            임베딩 레이어                                                          임베딩 전파 레이어                                                          유저-아이템 선호도 예측 레이어                                          L = 3~4일 때 가장 좋은 성능      MF보다 빠르게 수렴하고 recall이 높음        LightGCN          이웃노드의 임베딩을 가중합      concat 하지 않고 가중합으로 대체하여 연산량 감소      training loss와 추천 성능 모두 NGCF보다 뛰어남        GRU4Rec          고객이 지금 좋아하는 것 찾기      세션을 GRU 레이어에 입력하여 다음에 올 확률이 높은 아이템 추천      길이가 짧은 세션과 긴 세션을 묶어 병렬적으로 구성하여 mini-batch 학습      아이템을 negative sampling하여 subset만으로 loss 계산      상호작용하지 않은 아이템은 몰랐거나 관심이 없는 것      Context-aware Recommendation  MF 기반 CF는 상호작용 정보가 부족할 때(cold start)에 대한 대처가 어려움  상호작용 정보 뿐만 아니라 맥락 정보도 함께 반영  CTR 예측 등에 사용  sparse feature에 매우 좋은 성능FM  Factorization Machine  \\[\\hat{y}(x) = w_{0} + \\sum_{i=1}^{n}w_{i}x_{i} + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} &lt;v_{i}, v_{j}&gt;x_{i}x_{j}\\]    MF와 다르게 범용적인 지도 학습 모델  유저, 아이템 id외에 다른 정보도 모델의 feature로 사용 가능FFM  Field-aware FM  \\[\\hat{y}(x) = w_{0} + \\sum_{i=1}^{n}w_{i}x_{i} + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} &lt;v_{i, f_j}, v_{j, f_i}&gt;x_{i}x_{j}\\]    하나의 변수에 대해 필드 개수(f)와 factorization 차원(k)의 곱(fk)만큼의 파라미터 학습 (FM은 k 파라미터)GBM  Gradient Boosting Machine  하이퍼파라미터에 비교적 민감하지 않은 robust한 모델  이전 단계 weak learner까지의 잔차(residual)을 계산하여 다음 weak learner를 학습함  회귀문제          residual        분류문제          log(odds)        장점          random forest보다 나은 성능        단점          느린 학습 속도      과적합 문제        GBM의 단점을 해결하기 위한 모델 / 라이브러리          XGBoost                  병렬처리 / 근사 알고리즘 사용으로 학습 속도 개선                    LightGBM                  병렬처리 없이도 빠르게 학습                    CatBoost                  범주형 변수에 효과적                    "
  },
  
  {
    "title": "Day20",
    "url": "/posts/Day20/",
    "categories": "BoostCamp, Week4",
    "tags": "dl, data, recsys, visualization",
    "date": "2023-12-01 20:00:00 +0900",
    





    
    "snippet": "Item2Vec, ANN, DL RecSys 정리  Day18스페셜 피어세션피어세션  Generative Model 리뷰주간회고  이번주부터 처음으로 RecSys에 관련된 내용을 학습했다  내용들이 전부 처음보는 거라 주말에 진짜 복습을 좀 해서 정리를 해야겠다",
    "content": "Item2Vec, ANN, DL RecSys 정리  Day18스페셜 피어세션피어세션  Generative Model 리뷰주간회고  이번주부터 처음으로 RecSys에 관련된 내용을 학습했다  내용들이 전부 처음보는 거라 주말에 진짜 복습을 좀 해서 정리를 해야겠다"
  },
  
  {
    "title": "Day19",
    "url": "/posts/Day19/",
    "categories": "BoostCamp, Week4",
    "tags": "dl, data, recsys, visualization, plotly, bokeh, altair",
    "date": "2023-11-30 20:00:00 +0900",
    





    
    "snippet": "기본 과제 2 마무리  It works…? Why?Data Viz  Polar Coordinate  Pie Chart  시각화 라이브러리  Interactive Visualization          Plotly      Plotly Express      Bokeh      Altair        Custom Matplotlib Theme  Im...",
    "content": "기본 과제 2 마무리  It works…? Why?Data Viz  Polar Coordinate  Pie Chart  시각화 라이브러리  Interactive Visualization          Plotly      Plotly Express      Bokeh      Altair        Custom Matplotlib Theme  Image &amp; Text Visualization Techniques피어세션  transformer 리뷰내일은 18일차 정리 안된거 정리하기"
  },
  
  {
    "title": "Day18",
    "url": "/posts/Day18/",
    "categories": "BoostCamp, Week4",
    "tags": "dl, git, recsys, item2vec, ann",
    "date": "2023-11-29 20:00:00 +0900",
    





    
    "snippet": "기본 과제 1  Matrix Factorization  Alternative Least Squares피어세션  RNN, LSTM 리뷰Item2Vec  CBOW(Continuous Bag of Words)          앞 뒤로 각각 n개의 단어 슬라이딩 윈도우 형태로        SG(Skip Gram)          CBOW의 입력층과 출력층이 ...",
    "content": "기본 과제 1  Matrix Factorization  Alternative Least Squares피어세션  RNN, LSTM 리뷰Item2Vec  CBOW(Continuous Bag of Words)          앞 뒤로 각각 n개의 단어 슬라이딩 윈도우 형태로        SG(Skip Gram)          CBOW의 입력층과 출력층이 바뀐 모델      CBOW보다 성능이 좋다고 알려져 있음        SGNS(Skip Gram with Negative Sampling)          Item2Vec에서 사용하는 학습 방법      SG의 입력과 레이블을 모두 입력값으로      레이블을 이진 분류(주변 단어는 1, 그 외는 0)      0의 개수는 Hyperparameter(학습데이터가 적은 경우 5~20, 큰 경우 2~5)      중심 단어 layer, 주변 단어 layer 2개의 embedding layer 존재        Item2Vec          공간적 / 시간적 정보를 무시      동일한 아이템 집합 내 아이템 쌍들은 모두 SGNS의 Positive Sample이 됨      Skip-Gram이 n개의 단어를 사용한 것과 달리 모든 단어 쌍을 사용      ANN  Approximate Nearest Neighbor  Brute Force KNN          정확도는 100프로지만 시간이 오래걸림        ANNOY          spotify에서 개발한 tree based ANN      parameter를 통해 accuarcy ↔ speed trade-off 조정 가능 (num_tree, search_k)                                            임의의 두 점 선택 → 두 점 사이의 hyperplane으로 vector space 분리                                                          subspace의 점들을 node로 하여 binary tree 생성 or 갱신                                                          subspace 내의 점들이 K개 이상이라면 1~2 반복                                          문제점                  가장 근접한 점이 tree의 다른 node에 있을 경우 해당 점은 후보 subset에 포함되지 못함                    해결 방안                  priority queue 사용하여 가까운 다른 node 탐색          binary tree를 여러 개 생성하여 병렬적으로 탐색                      HNSW(Hierarchical Navigable Small World Graphs)          벡터를 그래프의 node로 표현하고 인접한 벡터를 edge로 연결      nmslib, faiss 등                                            최상위 layer에서 임의의 노드에서 시작                                                          현재 layer에서 타겟 노드와 가장 가까운 노드로 이동                                                          현재 layer에서 더 가까워 질 수 없으면 하위 layer로 이동                                                          타켓 노드에 도착할 때 까지 2~3 반복                                                          2~4를 진행했을 때 방문했던 노드들만 후보로 하여 NN 탐색                                            IVF(Inverted File Index)          탐색해야 하는 cluster 개수를 증가시킬수록 accuracy ↔ speed trade-off 발생                                            주어진 vector를 clustering을 통해 n개의 cluster로 나눠서 저장                                                          vector의 index를 cluster별 inverted list로 저장                                                          query vector에 대해서 해당 cluster를 찾고 invert list 안에 있는 vector들에 대해 탐색                                            Product Quantization - Compression          두 vector의 유사도를 구하는 연산이 거의 요구되지 않음      centroid 사이의 유사도 이용      PQ와 IVF를 동시에 사용해서 더 빠르고 효율적인 ANN 수행 (faiss)                                            기존 vector를 n개의 sub-vector로 나눔                                                          각 sub-vecotr 군에 대해 k-mens clustering을 통해 centroid를 구함                                                          기존의 모든 vector를 n개의 centroid로 압축해서 표현                                          DL RecSys  추천 시스템에서 딥러닝을 사용하는 이유          Nonlinear Transformation      Representation Learning      Sequence Modeling      Flexibility        NCF(Neural Collaborative Filtering)          MF의 한계 ← 선형 조합      user / item latent vector → MLP        YouTube Recommendation                            Candidate Generation                          High Recall이 목표              Top N 추천 아이템 생성                                                            Ranking                          유저, 비디오 feature를 좀 더 풍부하게 사용              logistic 회귀 사용              시청 시간을 가중치로                                            AutoRec          AE를 CF에 적용      Rating Vector를 입력과 출력으로 Encoder &amp; Decoder Reconstruction 수행      non-linear activation function을 사용하므로 더 복잡한 interaction 표현 가능      아이템 / 유저 둘 중 한 번에 하나에 대한 임베딩만을 진행        CDAE(Collaborative Denoising Auto-Encoder)          AutoRec이 Rating Predicion이었다면 CDAE는 Top-N 추천      유저-아이템 상호작용 정보를 0 or 1로 바꿔서 학습데이터로 사용      기본 과제 2  Neural Collaborative Filtering  AutoRec"
  },
  
  {
    "title": "Day17",
    "url": "/posts/Day17/",
    "categories": "BoostCamp, Week4",
    "tags": "dl, git, recsys, cf, nbcf, knn cf, latent, svd, mf, als, bpr",
    "date": "2023-11-28 20:00:00 +0900",
    





    
    "snippet": "생활코딩 egoing님 git 특강  복구          프로젝트 폴더 전체 복사      커밋 id 복사 해두기      위 두개 대신 “branch”        깃 작업 로그 보기          git reflog        merge          원시 코드 base와 3자 대면                  둘 다 수정 안한 코드 → ...",
    "content": "생활코딩 egoing님 git 특강  복구          프로젝트 폴더 전체 복사      커밋 id 복사 해두기      위 두개 대신 “branch”        깃 작업 로그 보기          git reflog        merge          원시 코드 base와 3자 대면                  둘 다 수정 안한 코드 → 그대로          하나만 수정한 코드 → 수정한거          둘 다 수정한 코드 → 사람이 해결(현재 사항 / 수신 사항 / 모두 수락 / 무시 등)                    피어세션  CNN 리뷰Collaborative Filtering      NBCF                  IBCF                    UBCF                  KNN CF                  KNN Similarity Measure                  MSD(Mean Squared Difference)          Cosine          Pearson          Jaccard 등등                          Rating Prediction                  UBCF                  Absolute Rating                          Average              Weighted Average                                Relative Rating                          Using Deviation                                                  IBCF                  Weighted Average          Using Deviation                          Latent Factor Model          저차원의 행렬로 분해하여 같은 벡터공간으로 만든 후 유사도를 평가            MBCF          모델 학습 / 서빙 빠름      Sparsity / Scalability 개선      Overfitting 방지      Limited Coverage 극복              SVD(Singular Value Decomposition)                  유저 잠재 요인 행렬          잠재 요인 대각 행렬          아이템 잠재 요인 행렬                      Full SVD\\[R = U\\sum\\limits V^T\\]                                Truncated SVD\\[R \\approx \\hat{U} \\sum {_k\\widehat{V^{T}}}= \\hat{R}\\]                    행렬의 Knowledge가 불완전할 때 정의 X          결측치를 모두 채워 Dense Matrix를 만들어서 SVD를 수행하면                          데이터 양 증가, computation 비용 증가                                정확하지 않은 결측치 Imputation은 데이터를 왜곡하고 예층 성능을 저하시킴                          entry가 매우 적을때 SVD 사용하면 과적합                                                  MF(Matrix Factorization)                  User-Item 행렬을 저차원의 User, Item의 latent factor matrix의 곱으로 분해                      관측된 선호도만 모델링에 활용\\[\\begin{aligned}    &amp;R \\approx P \\times Q^{T}= \\hat{R} \\\\    &amp;P \\rightarrow \\left| U \\right| \\times k \\\\    &amp;Q \\rightarrow \\left| I \\right| \\times k  \\end{aligned}\\]                                      ALS(Alternative Least Square)                  User, Item matrix 번갈아가며 업데이트          하나를 고정시키고 다른하나로 least-square          SGD보다 robust하고 병렬처리로 빠른 학습가능                          BPR(Bayesian Personalized Ranking)          item i보다 j를 좋아한다면 더 높은 ranking      관측되지 않은 데이터에 대해                  유저가 아이템에 대해 관심이 없는 것인지          유저가 실제로 관심이 있지만 아직 모르는 것인지 고려                    가정                  관측된 아이템은 관측되지 않은 아이템보다 선호          관측된 아이템끼리는 선호도 추론 불가          관측되지 않은 아이템끼리는 선호도 추론 불가                  \\[\\begin{aligned}    &amp;p(\\theta|_{u}) \\propto p(&gt;_{u}|\\theta)p(\\theta) \\\\    p(\\theta) =\\ 파라미터에\\ 대한&amp;\\ 사전\\ 정보\\ (prior) \\\\    p(&gt;_{u}|\\theta) =\\ 주어진\\ 파라&amp;미터에\\ 대한\\ 유저의\\ 선호\\ 정보의\\ 확률\\ (likelihood) \\\\    p(\\theta|&gt;_{u}) =\\ 주어진\\ 유저&amp;의\\ 선호\\ 정보에\\ 대한\\ 파라미터의\\ 확률\\ (posterior)  \\end{aligned}\\]  "
  },
  
  {
    "title": "Day16",
    "url": "/posts/Day16/",
    "categories": "BoostCamp, Week4",
    "tags": "dl, data, visualization, recsys",
    "date": "2023-11-27 20:00:00 +0900",
    





    
    "snippet": "Data Viz  SeabornRecSys Basic  추천 시스템 개요          Explicit Feedback / Implicit Feedback      Ranking / Prediction        Offline Test          고전적인 통계학 지표      Ranking                  Precision, R...",
    "content": "Data Viz  SeabornRecSys Basic  추천 시스템 개요          Explicit Feedback / Implicit Feedback      Ranking / Prediction        Offline Test          고전적인 통계학 지표      Ranking                  Precision, Recall, MAP, NDCG, Hit Rate                    Prediction                  RMSE, MAE                      Online Test          A/B Test        연관분석  TF-IDF를 활용한 CB(Content-based Recommendation)피어세션  DL Basic의 MLP 리뷰마스터클래스  안수빈 마스터님의 &lt;The Journey of a Data Analyst&gt;"
  },
  
  {
    "title": "Day15",
    "url": "/posts/Day15/",
    "categories": "BoostCamp, Week3",
    "tags": "dl, data, visualization",
    "date": "2023-11-24 20:00:00 +0900",
    





    
    "snippet": "부족한 부분 복습스페셜 피어세션피어세션  주간 회고주간 학습 회고  이번 주는 생긴 의문점들이 많았다.  해결된 것도 있고 안 된 것도 있어서 주말에 좀 더 고민해 봐야겠다.",
    "content": "부족한 부분 복습스페셜 피어세션피어세션  주간 회고주간 학습 회고  이번 주는 생긴 의문점들이 많았다.  해결된 것도 있고 안 된 것도 있어서 주말에 좀 더 고민해 봐야겠다."
  },
  
  {
    "title": "Day14",
    "url": "/posts/Day14/",
    "categories": "BoostCamp, Week3",
    "tags": "dl, data, visualization",
    "date": "2023-11-23 20:00:00 +0900",
    





    
    "snippet": "Data Viz  Text  Color  Facet  More Tips피어세션  심화 과제 1 &amp; 2 리뷰마스터 클래스  최성준 마스터님의 &lt;Learning by Teaching&gt;  가르치는게 머리 속에 가장 오래 남는다",
    "content": "Data Viz  Text  Color  Facet  More Tips피어세션  심화 과제 1 &amp; 2 리뷰마스터 클래스  최성준 마스터님의 &lt;Learning by Teaching&gt;  가르치는게 머리 속에 가장 오래 남는다"
  },
  
  {
    "title": "Matrix Multiply",
    "url": "/posts/Matrix_Multiply/",
    "categories": "BoostCamp, 궁금증",
    "tags": "numpy, matmul",
    "date": "2023-11-22 20:00:00 +0900",
    





    
    "snippet": "다차원에서의 행렬곱  행렬 곱에 대해서 고민하며 시도해보다가 (2, 1, 2, 3, 5)와 (1, 4, 1, 5, 3)가 곱해진다는 것을 알고 혼란에 빠져 버렸다.  이를 해결하기 위해 chat gpt에게 물어본 결과다.BroadcastingNumPy에서는 서로 다른 크기의 배열 간에도 행렬 연산을 수행할 수 있는 기능인 broadcasting이 제공...",
    "content": "다차원에서의 행렬곱  행렬 곱에 대해서 고민하며 시도해보다가 (2, 1, 2, 3, 5)와 (1, 4, 1, 5, 3)가 곱해진다는 것을 알고 혼란에 빠져 버렸다.  이를 해결하기 위해 chat gpt에게 물어본 결과다.BroadcastingNumPy에서는 서로 다른 크기의 배열 간에도 행렬 연산을 수행할 수 있는 기능인 broadcasting이 제공됩니다. Broadcasting은 배열의 차원을 자동으로 조정하여 연산이 가능하도록 만듭니다. 특히, 크기가 1인 차원을 다른 배열의 차원에 맞춰 확장시키는 것이 주요한 원리입니다.다음은 broadcasting이 가능한 경우와 그 원리에 대한 설명입니다:  차원 수가 다른 경우:          broadcasting은 차원 수가 다르더라도 가능합니다. 누락된 차원은 크기가 1로 간주됩니다.        한 배열의 차원 크기가 다른 배열의 해당 차원 크기에 1이 있는 경우:          차원 크기가 1인 차원은 다른 배열의 해당 차원 크기에 맞춰 확장됩니다.      예를 들어, x 배열이 shape이 (2, 1, 2, 1)이고 y 배열이 shape이 (1, 4, 1, 1)인 경우, broadcasting이 가능합니다. 이 경우, x의 두 번째 차원과 y의 두 번째 차원이 크기 1에서 서로 맞춰져 확장됩니다. 따라서 x @ y에서 두 배열 간의 행렬 곱셈이 가능합니다.이런 방식으로 broadcasting이 작동하면서 두 배열 간의 연산이 가능하게 됩니다. NumPy는 이러한 broadcasting을 통해 편리하게 다양한 형태의 배열 간 연산을 수행할 수 있도록 지원합니다.예시두 5차원 배열 A와 B, 그리고 5차원 배열 C에 대한 행렬 곱셈을 예시로 들어보겠습니다. 각 배열의 차원과 크기를 명시하겠습니다.\\[D_{ijkln} = \\sum_{m} A_{ijmkl} \\cdot B_{mklno} \\cdot C_{opqnr}\\]여기서:  ( A )의 shape: (2, 3, 4, 5, 6)  ( B )의 shape: (3, 4, 2, 6, 1)  ( C )의 shape: (4, 2, 3, 1, 5)이제 곱셈을 수행하는 과정을 자세히 설명하겠습니다.과정  D의 첫 번째 차원 (i에 대한 차원):          ( A )의 첫 번째 차원과 ( B )의 첫 번째 차원 중 크기가 더 큰 값으로 결정합니다. 따라서 ( D )의 첫 번째 차원은 3입니다.        D의 두 번째 차원 (j에 대한 차원):          ( B )의 두 번째 차원이 그대로 사용됩니다. 따라서 ( D )의 두 번째 차원은 4입니다.        D의 세 번째 차원 (k에 대한 차원):          ( A )의 세 번째 차원과 ( C )의 첫 번째 차원 중 크기가 더 큰 값으로 결정합니다. 따라서 ( D )의 세 번째 차원은 4입니다.        D의 네 번째 차원 (l에 대한 차원):          ( A )의 네 번째 차원이 그대로 사용됩니다. 따라서 ( D )의 네 번째 차원은 5입니다.        D의 다섯 번째 차원 (n에 대한 차원):          ( C )의 다섯 번째 차원이 그대로 사용됩니다. 따라서 ( D )의 다섯 번째 차원은 5입니다.      결론따라서, ( D )의 shape은 (3, 4, 4, 5, 5)가 됩니다. 이제 ( D )의 각 원소를 구하는데 필요한 곱셈을 수행하여 최종 결과를 얻을 수 있습니다. 이 과정은 각 차원에 대해 대응하는 크기를 고려하여 수행됩니다.  아직도 100퍼센트 이해가 안된것같다…  해당 내용을 코드로 바꿔서 돌려보니 작동하지 않는다. chat gpt 3.5로 돌려서 지 멋대로 내용을 만들어 낸듯하다…제대로 알아내면 올릴 곳…해당 내용에 대해 더 찾아보고 수정해야겠다"
  },
  
  {
    "title": "Day13",
    "url": "/posts/Day13/",
    "categories": "BoostCamp, Week3",
    "tags": "data, visualization",
    "date": "2023-11-22 20:00:00 +0900",
    





    
    "snippet": "Data Viz  시각화의 요소들  Matplotlib  Bar plot  line plot  scatter plot멘토링  RecSys의 validation          offline validation                  top K, precision &amp; recall …                    online valid...",
    "content": "Data Viz  시각화의 요소들  Matplotlib  Bar plot  line plot  scatter plot멘토링  RecSys의 validation          offline validation                  top K, precision &amp; recall …                    online validation                  A/B test                      재노출의 설정          domain에 따라 class에 따라 후처리를 통해 빈도 설정                  우유는 1주일, 옷은 구매하면 x          상황마다 회사마다 다 다르다                    피어세션  기본과제 4 &amp; 5 리뷰"
  },
  
  {
    "title": "행렬 곱에서의 transpose",
    "url": "/posts/matmul_transpose/",
    "categories": "BoostCamp, 궁금증",
    "tags": "dl, transformer, score, T, transpose, permute",
    "date": "2023-11-21 20:00:00 +0900",
    





    
    "snippet": "transformer score 파트에서 생긴 의문들",
    "content": "transformer score 파트에서 생긴 의문들"
  },
  
  {
    "title": "Day12",
    "url": "/posts/Day12/",
    "categories": "BoostCamp, Week3",
    "tags": "dl",
    "date": "2023-11-21 20:00:00 +0900",
    





    
    "snippet": "TransformerGenerative Model과제 5  score 파트에서 생긴 의문          행렬 곱에서의 transpose      깃허브  post link  image upload          use cdn        sitemap 재지정  naver 서치어드바이저 등록",
    "content": "TransformerGenerative Model과제 5  score 파트에서 생긴 의문          행렬 곱에서의 transpose      깃허브  post link  image upload          use cdn        sitemap 재지정  naver 서치어드바이저 등록"
  },
  
  {
    "title": "Transfer Learning에서의 의문",
    "url": "/posts/transfer_learning/",
    "categories": "BoostCamp, 궁금증",
    "tags": "dl, transfer, transfer learning",
    "date": "2023-11-20 20:00:00 +0900",
    





    
    "snippet": "transfer learning의 layer 수정 이유",
    "content": "transfer learning의 layer 수정 이유"
  },
  
  {
    "title": "Day11",
    "url": "/posts/Day11/",
    "categories": "BoostCamp, Week3",
    "tags": "dl",
    "date": "2023-11-20 20:00:00 +0900",
    





    
    "snippet": "DL Historical reviewNN &amp; MLPOptimization  Generalization  Under-fitting vs Over-fitting  Cross Validation  Bias-Variance tradeoff  Bootstrapping  Bagging &amp; BoostingCNNRNN  RNN  LSTM  GRU피어세...",
    "content": "DL Historical reviewNN &amp; MLPOptimization  Generalization  Under-fitting vs Over-fitting  Cross Validation  Bias-Variance tradeoff  Bootstrapping  Bagging &amp; BoostingCNNRNN  RNN  LSTM  GRU피어세션  심화과제1 리뷰  transfer learning에서 layer를 왜 그렇게 수정했는지          transfer learning      과제 1~4  MLP, Optimization, CNN, LSTM"
  },
  
  {
    "title": "Day10",
    "url": "/posts/Day10/",
    "categories": "BoostCamp, Week2",
    "tags": "pytorch",
    "date": "2023-11-17 20:00:00 +0900",
    





    
    "snippet": "심화 과제 1  계속 시도 중… 이지만 어렵다스페셜 피어세션  새로운분들을 만나고 관심사에 대해 알아보는 시간피어세션  스페셜 피어세션에 대한 이야기  팀 주간 학습회고주간 학습 회고  이번 주는 생각보다 처음 경험하는 내용도 많았고 배운것도 많은 것 같다.  특히 gather, forward, backward, hook, apply는 처음 배운 내용...",
    "content": "심화 과제 1  계속 시도 중… 이지만 어렵다스페셜 피어세션  새로운분들을 만나고 관심사에 대해 알아보는 시간피어세션  스페셜 피어세션에 대한 이야기  팀 주간 학습회고주간 학습 회고  이번 주는 생각보다 처음 경험하는 내용도 많았고 배운것도 많은 것 같다.  특히 gather, forward, backward, hook, apply는 처음 배운 내용들이라 체득하는데 오래 걸린 것 같다.깃 블로그 업데이트  jeykll theme에서 toc를 써보고 싶은데 계속 안되서 고민중…          Heading을 # 하나가 아니라 ## 부터 toc 정상 적용!      "
  },
  
  {
    "title": "Day9",
    "url": "/posts/Day9/",
    "categories": "BoostCamp, Week2",
    "tags": "pytorch",
    "date": "2023-11-16 20:00:00 +0900",
    





    
    "snippet": "Multi GPU  Single Node Single GPU  Single Node Multi GPU  Multi Node Multi GPU  Model Parallel  Data Parallel          Data Parallel      DistributedData Parallel      Hyperparameter Tuning  Hyperp...",
    "content": "Multi GPU  Single Node Single GPU  Single Node Multi GPU  Multi Node Multi GPU  Model Parallel  Data Parallel          Data Parallel      DistributedData Parallel      Hyperparameter Tuning  Hyperparameter Tuning(마지막에 쥐어짤 때)          Grid Layout      Random Layout      Bayesian Layout(최근)        Ray          Multi Node Multi GPU 지원      PyTorch Troubleshooting  OOM(Out of Memory)          Batch Size ↓ → GPU clean → Run      torch.cuda.empty_cache()      del      tensor to list      inference → torch.no_grad()                  backward pass에서의 메모리에서 자유로움                    기본 과제 2 마무리피어세션  기본 과제 2 리뷰마스터클래스  최성철 마스터님의 &lt;ChatGPT시대&gt;  ChatGPT 많이 활용하기"
  },
  
  {
    "title": "Day8",
    "url": "/posts/Day8/",
    "categories": "BoostCamp, Week2",
    "tags": "pytorch",
    "date": "2023-11-15 20:00:00 +0900",
    





    
    "snippet": "PyTorch 모델 불러오기  save  checkpoint  transfer learningMonitoring tools  Tensorboard  Wandb기본 과제 2멘토링  논문 읽는 법에 대하여피어세션  기본 과제 1 리뷰두런두런 1회차  변성윤 마스터님 &lt;어쩌다 데이터 사이언티스트&gt;  나는 나 자신을 얼마나 알고 있는가?",
    "content": "PyTorch 모델 불러오기  save  checkpoint  transfer learningMonitoring tools  Tensorboard  Wandb기본 과제 2멘토링  논문 읽는 법에 대하여피어세션  기본 과제 1 리뷰두런두런 1회차  변성윤 마스터님 &lt;어쩌다 데이터 사이언티스트&gt;  나는 나 자신을 얼마나 알고 있는가?"
  },
  
  {
    "title": "Day7",
    "url": "/posts/Day7/",
    "categories": "BoostCamp, Week2",
    "tags": "pytorch",
    "date": "2023-11-14 20:00:00 +0900",
    





    
    "snippet": "Pytorch AutoGrad &amp; Optimizer  nn.Module  nn.Parameter  forward  backwardPyTorch Dataset &amp; DataLoader  transforms / Dataset / DataLoader 모듈피어세션  기본 과제 1 (~nn.Module 알쓸신접 전까지) 리뷰기본 과제 1 마무리",
    "content": "Pytorch AutoGrad &amp; Optimizer  nn.Module  nn.Parameter  forward  backwardPyTorch Dataset &amp; DataLoader  transforms / Dataset / DataLoader 모듈피어세션  기본 과제 1 (~nn.Module 알쓸신접 전까지) 리뷰기본 과제 1 마무리"
  },
  
  {
    "title": "Day6",
    "url": "/posts/Day6/",
    "categories": "BoostCamp, Week2",
    "tags": "pytorch",
    "date": "2023-11-13 20:00:00 +0900",
    





    
    "snippet": "PyTorch  basic  구조 이해피어세션  심화과제 1 &amp; 2 &amp; 3  기본 과제 1 (nn.Module 알쓸신잡 전까지)기본 과제 1  상당히 오래걸렸지만 새로 알아가는 것이 많았다.",
    "content": "PyTorch  basic  구조 이해피어세션  심화과제 1 &amp; 2 &amp; 3  기본 과제 1 (nn.Module 알쓸신잡 전까지)기본 과제 1  상당히 오래걸렸지만 새로 알아가는 것이 많았다."
  },
  
  {
    "title": "Day5",
    "url": "/posts/Day5/",
    "categories": "BoostCamp, Week1",
    "tags": "ai math",
    "date": "2023-11-10 15:00:00 +0900",
    





    
    "snippet": "심화 과제 1  경사하강법에 대해 좀 더 깊게 알아가는 시간깃블로그 업데이트  아직 미숙한 관계로 더 공부 후에 업데이트 해야겠다…주간 학습 회고  첫주차는 알았던 내용의 복습 &amp; 놓치고 있던 몇몇 부분을 찾는 시간이었다  주말에는 휴식을 가지면서 부족한 부분과 github.io에 대해 좀 더 공부해 봐야겠다",
    "content": "심화 과제 1  경사하강법에 대해 좀 더 깊게 알아가는 시간깃블로그 업데이트  아직 미숙한 관계로 더 공부 후에 업데이트 해야겠다…주간 학습 회고  첫주차는 알았던 내용의 복습 &amp; 놓치고 있던 몇몇 부분을 찾는 시간이었다  주말에는 휴식을 가지면서 부족한 부분과 github.io에 대해 좀 더 공부해 봐야겠다"
  },
  
  {
    "title": "Day4",
    "url": "/posts/Day4/",
    "categories": "BoostCamp, Week1",
    "tags": "ai math",
    "date": "2023-11-09 20:00:00 +0900",
    





    
    "snippet": "경사하강법  미분 / gradient vector  gradient descent / stochastic gradient descent(minibatch)    딥러닝    softmax(classification)  activation function  multi-layer perceptron  backprogpagation(chian rule)확률...",
    "content": "경사하강법  미분 / gradient vector  gradient descent / stochastic gradient descent(minibatch)    딥러닝    softmax(classification)  activation function  multi-layer perceptron  backprogpagation(chian rule)확률론  discrete(이산형) / continuous(연속형)  조건부확률  몬테카를로 샘플링(확률 분포를 모를 때)통계학  MLE / Log-likelihood          쿨백-라이블러 발산(KL Divergence)        베이즈 정리  precision / recallCNN  convolution 연산 / convolution 연산의 역전파RNN  시퀀스 데이터  vanishing gradient -&gt; GRU / LSTM피어세션  1 주차 학습내용 리뷰  과제 1 &amp; 2 &amp; 3 리뷰마스터클래스  임성빈 마스터님의 &lt;AI &amp; Math FAQ&gt;  AI를 배우는 사람에게 황금같은 조언"
  },
  
  {
    "title": "Day3",
    "url": "/posts/Day3/",
    "categories": "BoostCamp, Week1",
    "tags": "python, ai math",
    "date": "2023-11-08 20:00:00 +0900",
    





    
    "snippet": "Python Data Handling  csv / html / xml / json넘파이판다스  series / dataframe  lambda / map / apply  pandas built-in functions  groupby / crosstab / merge / concat  persistence벡터  basic operation  Norm  ...",
    "content": "Python Data Handling  csv / html / xml / json넘파이판다스  series / dataframe  lambda / map / apply  pandas built-in functions  groupby / crosstab / merge / concat  persistence벡터  basic operation  Norm  inner product행렬  basic operation  inner product  inv / pinv          연립방정식 / linear regression      기본 과제 2 &amp; 3피어세션  데이터 전처리 (Filtering &amp; Sorting) 리뷰  새로 오신 팀원분과 자기소개과제 및 퀴즈 리마인드"
  },
  
  {
    "title": "Day2",
    "url": "/posts/Day2/",
    "categories": "BoostCamp, Week1",
    "tags": "python",
    "date": "2023-11-07 20:00:00 +0900",
    





    
    "snippet": "파이썬 데이터 구조파이썬 코드  python style code          ex) split &amp; join, list comprehension…      파이썬 객체지향 프로그래밍  Class          Inheritance / Polymorphism / Visibility        decorator모듈 / 프로젝트  import ...",
    "content": "파이썬 데이터 구조파이썬 코드  python style code          ex) split &amp; join, list comprehension…      파이썬 객체지향 프로그래밍  Class          Inheritance / Polymorphism / Visibility        decorator모듈 / 프로젝트  import / namespace  Virtual Environment파일 / 예외 처리 / 로그  configparser / argparser기본 과제 1피어세션  데이터 전처리 (Getting &amp; Knowing) 리뷰  팀 그라운드 룰 설정데이터 전처리  Filtering &amp; Sorting"
  },
  
  {
    "title": "Day1",
    "url": "/posts/Day1/",
    "categories": "BoostCamp, Week1",
    "tags": "python",
    "date": "2023-11-06 20:00:00 +0900",
    





    
    "snippet": "파이썬 개발환경 설정  vscode / anaconda / terminal 등파이썬 기초 문법  variable / function / conditional &amp; loop / string피어세션  자기소개  Datamanim 사이트 이용 -&gt; 전처리 관련 학습 계획데이터 전처리  Getting &amp; Knowing깃블로그 생성  them...",
    "content": "파이썬 개발환경 설정  vscode / anaconda / terminal 등파이썬 기초 문법  variable / function / conditional &amp; loop / string피어세션  자기소개  Datamanim 사이트 이용 -&gt; 전처리 관련 학습 계획데이터 전처리  Getting &amp; Knowing깃블로그 생성  theme / google search console / google ad sense"
  }
  
]

