---
title: Day27
date: 2023-12-12 20:00:00 +09:00
categories: [BoostCamp, Week6]
tags: [dl, data, recsys]     # TAG names should always be lowercase
toc: true
toc_sticky: true
use_math: true
---

## Model Based CF
- ### Unsupervised Learning Model
	- #### Latent Factor Model
		- 유저와 아이템 사이 다양한 정보를 담은 데이터를 축약된 벡터 공간에 표현
		- ##### SVD
			- 차원 축소 용도로도 많이 활용되는 SVD를 이용
			- 행렬을 분해 &rarr; 차원 축소 &rarr; 다시 복구
			- $R=U\sum\limits V^{T}\qquad(Full SVD)$
			- $R\approx\hat{U}\sum_{k}\hat V^{T}=\hat R \qquad(Truncated SVD)$
			- 한계점
				- User와 Item이 많아지면 예측력 감소
				- 결측치가 많은 데이터에 정상 작동 x
					- 현실의 데이터는 결측치가 대부분
		- ##### MF
			- SVD는 $R=U\sum\limits V^{T}$로 3가지 행렬의 곱으로 표현
			- MF는 $R\approx PQ^T$로 User Latent Factor와 Item Latent Factor 2가지 행렬의 곱으로 표현
			- 목적함수
				- $\underset{P,Q}{min} \underset{observed\;r_{u,i}}{\sum\limits}(r_{u,i}-p_{u}q_{i}^{T})^2$
			- P,Q를 구하기 위해 SGD 활용
			- 과적합 방지를 위한 L2 정규화 term
				- $\underset{P,Q}{min} \underset{observed\;r_{u,i}}{\sum\limits}(r_{u,i}-p_{u}q_{i}^{T})^2+\lambda(\lVert p_{u}\rVert ^{2}+\lVert q_{i}\rVert ^2)$
			- 기존 편향된 정도를 반영하기 위해 bias term 추가
				- $\underset{P,Q}{min} \underset{observed\;r_{u,i}}{\sum\limits}(r_{u,i}-\mu-b_{u}-b_{i}-p_{u}q_{i}^{T})^2+\lambda(\lVert p_{u}\rVert ^{2}+\lVert q_{i}\rVert ^2)$
			- SGD를 이용한 MF의 한계점
				- p와 q를 변수를 가지므로 빠른 계산 불가능
				- SGD를 학습하는 과정에서 업데이트를 여러번 &rarr; 해당 단점 학습할 때마다 누적
				- u, i가 커질수록 반복문으로 인하여 연산량 증가
		- ##### ALS
			- q를 고정값으로 두고 p만 변수로 (p를 고정값으로 두고 q만 변수로)
			- $p_{u}=(Q^{T}Q+\lambda I)^{-1}Q^{T}r_{u}$
			- $q_{i}=(P^{T}P+\lambda I)^{-1}P^{T}r_{i}$
			- 여러 연산이 필요하지 않아 계산 속도 증가
			- element 단위가 아닌 행렬 단위 연산이 가능 &rarr; $u \times i$ 만큼 반복문을 돌아야하는 문제 해결
- ### Hybrid Recommender System
	- 여러가지 추천 방법을 사용
	- 하나의 접근법을 다른 접근법에 더하거나 하나의 model로 만드는 등의 방법이 존재
	- cold start, sparsity 같은 문제들을 효과적으로 극복 가능
		- CB + CF
		- Memory Based CF + Model Based CF
		- Clustering - Memory Based CF 등
	- #### Hybridization 기술
		- ##### Weighted
			- 서로 다른 추천의 점수를 <font color="#ffff00">가중 평균</font>
		- ##### Switching
			- 추천 시스템을 변경하며 추천
		- ##### Mixed
			- 서로 다른 추천에서 나오는 추천을 <font color="#ffff00">섞어서 추천</font>
		- ##### Feature Combination
			- 다른 추천에서 나온 <font color="#ffff00">특징들을 합쳐 하나의 추천 알고리즘</font>에서 사용
		- ##### Feature Augmentation
			- <font color="#ffff00">Feature Combination</font>을 여러번 연결
		- ##### Cascade
			- 상위 추천 &rarr; 하위 추천
		- ##### Meta-level
			- 추천 시스템 model이 입력으로 사용되는 model로 추천
			- model 자체를 입력으로 전달
- ### Supervised Learning Model
	- #### Linear(Logistic) Regression
	- #### Random Forest
	- #### Naive Bayes
		- 일반 Bayes 공식
			- $P(Y=k\mid X)=\frac{P(X\mid Y=k)P(Y=k)}{P(X)}$
		- 다변량 Bayes 공식
			- $P(Y=k\mid X_{1},X_{2},...,X_{n})=\frac{P(X_{1}\mid Y=k)P(X_{2}\mid Y=k)...P(X_{n}\mid Y=k)P(Y=k)}{P(X_{1},X_{2},...,X_{n})}$
		- X 변수가 서로에 대해 독립적이라는 가정
			-> $P(X_{1}\mid Y,X_{2},X_{3})P(X_{1}\mid Y,X_{3},X_{8},X_{10})$ 같은 조건부 수식 제거 가능
		- binary 변수 많을 때 (one-hot encoding) BernoulliNB 사용하면 좋음
	- #### GBDT (Gradient Boosting Decision Tree)
		- NB에서 독립이라고 가정했던 다른 변수와의 상호 작용을 반영
		- ##### XGBoost
			- Parallelization
				- 기존 GBDT의 순차적 업데이트 개선
			- Level-wise
				- depth-fist
			- Regularization
				- overfitting 개선
		- ##### LightGBM
			- Leaf-wise
				- information gain이 있는 곳으로만 분할
				- level-wise 보다 좀 더 빠름
				- 과적합에는 취약
			- GOSS (Gradient-based One-Side Sampling)
				- Top N개 데이터 인스턴스 + 랜덤 샘플링 데이터 인스턴스만 활용
				- undersampling 되어 데이터 인스턴스 수 감소
			- EFB (Exclusive Feature Bundling)
				- sparse한 feature가 많을 때 번들링 하는 EFB
				- 여러 feature를 하나의 feature인 것처럼 줄여 feature 수 감소
			- O(data * feature) &rarr; O(data_GOSS * Bundles)
		- ##### CatBoost
			- Level-wise
				- overfitting 방지
			- Ordered Boosting
				- n+1번째 order의 잔차를 계산할 때 n번째까지 데이터로 학습한 모델에 $x_{n+1}$을 넣어 $y_{n+1}$과 비교
			- Ordered TS(Target Statics)
				- n+1번째 order의 TS를 계산할 때 n번째까지 데이터의 statistics 정보를 이용
				- y값의 평균을 사용
			- 예측 시점에서 사용할 수 없는 데이터가 데이터 셋에 포함되지 않도록

## 피어세션
- Context-aware Recommendation 리뷰

## 부캠살롱
- E-commerce
- Data Analisys

## Git 특강
- Branch
- Commit
- Issue
- Pull Request
- Tag