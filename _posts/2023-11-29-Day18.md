---
title: Day18
date: 2023-11-29 20:00:00 +09:00
categories: [BoostCamp, Week4]
tags: [dl, git, recsys, item2vec, ann]     # TAG names should always be lowercase
toc: true
toc_sticky: true
---

## 기본 과제 1
- Matrix Factorization
- Alternative Least Squares

## 피어세션
- RNN, LSTM 리뷰

## Item2Vec
- CBOW(Continuous Bag of Words)
	- 앞 뒤로 각각 n개의 단어 슬라이딩 윈도우 형태로
- SG(Skip Gram)
	- CBOW의 입력층과 출력층이 바뀐 모델
	- CBOW보다 성능이 좋다고 알려져 있음
- SGNS(Skip Gram with Negative Sampling)
	- Item2Vec에서 사용하는 학습 방법
	- SG의 입력과 레이블을 모두 입력값으로
	- 레이블을 이진 분류(주변 단어는 1, 그 외는 0)
	- 0의 개수는 Hyperparameter(학습데이터가 적은 경우 5~20, 큰 경우 2~5)
	- 중심 단어 layer, 주변 단어 layer 2개의 embedding layer 존재
- Item2Vec
	- 공간적 / 시간적 정보를 무시
	- 동일한 아이템 집합 내 아이템 쌍들은 모두 SGNS의 Positive Sample이 됨
	- Skip-Gram이 n개의 단어를 사용한 것과 달리 모든 단어 쌍을 사용

## ANN
- Approximate Nearest Neighbor
- Brute Force KNN
	- 정확도는 100프로지만 시간이 오래걸림
- ANNOY
	- spotify에서 개발한 tree based ANN
	- parameter를 통해 accuarcy &harr; speed trade-off 조정 가능 (num_tree, search_k)
		- 1. 임의의 두 점 선택 &rarr; 두 점 사이의 hyperplane으로 vector space 분리
		- 2. subspace의 점들을 node로 하여 binary tree 생성 or 갱신
		- 3. subspace 내의 점들이 K개 이상이라면 1~2 반복
	- 문제점
		- 가장 근접한 점이 tree의 다른 node에 있을 경우 해당 점은 후보 subset에 포함되지 못함
	- 해결 방안
		- priority queue 사용하여 가까운 다른 node 탐색
		- binary tree를 여러 개 생성하여 병렬적으로 탐색
- HNSW(Hierarchical Navigable Small World Graphs)
	- 벡터를 그래프의 node로 표현하고 인접한 벡터를 edge로 연결
	- nmslib, faiss 등
		- 1. 최상위 layer에서 임의의 노드에서 시작
		- 2. 현재 layer에서 타겟 노드와 가장 가까운 노드로 이동
		- 3. 현재 layer에서 더 가까워 질 수 없으면 하위 layer로 이동
		- 4. 타켓 노드에 도착할 때 까지 2~3 반복
		- 5. 2~4를 진행했을 때 방문했던 노드들만 후보로 하여 NN 탐색
- IVF(Inverted File Index)
	- 탐색해야 하는 cluster 개수를 증가시킬수록 accuracy &harr; speed trade-off 발생
		- 1. 주어진 vector를 clustering을 통해 n개의 cluster로 나눠서 저장
		- 2. vector의 index를 cluster별 inverted list로 저장
		- 3. query vector에 대해서 해당 cluster를 찾고 invert list 안에 있는 vector들에 대해 탐색
- Product Quantization - Compression
	- 두 vector의 유사도를 구하는 연산이 거의 요구되지 않음
	- centroid 사이의 유사도 이용
	- PQ와 IVF를 동시에 사용해서 더 빠르고 효율적인 ANN 수행 (faiss)
		- 1. 기존 vector를 n개의 sub-vector로 나눔
		- 2. 각 sub-vecotr 군에 대해 k-mens clustering을 통해 centroid를 구함
		- 3. 기존의 모든 vector를 n개의 centroid로 압축해서 표현

## DL RecSys
- 추천 시스템에서 딥러닝을 사용하는 이유
	- Nonlinear Transformation
	- Representation Learning
	- Sequence Modeling
	- Flexibility
- NCF(Neural Collaborative Filtering)
	- MF의 한계 &larr; 선형 조합
	- user / item latent vector &rarr; MLP
- YouTube Recommendation
	- 1. Candidate Generation
		- High Recall이 목표
		- Top N 추천 아이템 생성
	- 2. Ranking
		- 유저, 비디오 feature를 좀 더 풍부하게 사용
		- logistic 회귀 사용
		- 시청 시간을 가중치로
- AutoRec
	- AE를 CF에 적용
	- Rating Vector를 입력과 출력으로 Encoder & Decoder Reconstruction 수행
	- non-linear activation function을 사용하므로 더 복잡한 interaction 표현 가능
	- 아이템 / 유저 둘 중 한 번에 하나에 대한 임베딩만을 진행
- CDAE(Collaborative Denoising Auto-Encoder)
	- AutoRec이 Rating Predicion이었다면 CDAE는 Top-N 추천
	- 유저-아이템 상호작용 정보를 0 or 1로 바꿔서 학습데이터로 사용

## 기본 과제 2
- Neural Collaborative Filtering
- AutoRec