---
title: Day23
date: 2023-12-06 20:00:00 +09:00
categories: [BoostCamp, Week5]
tags: [dl, data, recsys, wide&deep, deepfm, din, bst]     # TAG names should always be lowercase
toc: true
toc_sticky: true
---

## Wide & Deep
- ### Memorization
	- 같이 자주 등장하는 item / feature를 <font color="#ffff00">과거로부터 학습</font>
	- LR
- ### Generalization
	- 드물거나 전혀 발생한 적 없는 item / feature를 <font color="#ffff00">기존 관계로부터 발견</font>
	- FM, DNN
- 두 특성을 가지는 구조를 결합
- ### The Wide Component
	- Generalized Linear Model
		- $$y = w^Tx + b$$
	- Cross-Product Transformation
		- $$\phi_{k}(x) = \prod_{i=1}^{d}x_{i}^{c_{ki}} \qquad c_{ki} \in \{0, 1\}$$
- ### The Deep Component
	- Feed-Forward Neural Network
		- 3 layer (ReLU)
		- 연속형 변수 &rarr; 그대로 사용
		- 범주형 변수 &rarr; feature embedding 후 사용
- $$P(Y = 1|x) = \sigma(W_{wide}^{T}[x, \phi(x)] + W_{deep}^{T}a^{(lf)} + b)$$
- Wide model은 offline / Deep model은 online에서 좋은 현상을 보이므로 두 모델을 결합하여 모두 좋은 성능

## DeepFM
- Wide & Deep model의 wide component는 feature engineering(Cross-Product Transformation)이 필요하다는 단점이 있음
- wide component로 FM을 사용하여 deep component의 <font color="#ffff00">dense embedding 입력값을 공유</font>
- DeepFM = FM + DNN
- $$\hat y = sigmoid(y_{FM}+y_{DNN})$$
![DeepFM.png](https://cdn.jsdelivr.net/gh/lostdesire/lostdesire.github.io/_posts/image/DeepFM.png)

## DIN(Deep Interest Network)
- 다양한 관심사를 반영하기 위해 나온 모델
	1. Embedding Layer
	2. <font color="#ff0000">Local Activation Layer</font>
		- 후보군이 되는 광고를 기존에 본 광고들의 <font color="#ffff00">연관성을 계산</font>하여 <font color="#ffff00">가중치</font>로 표현
	3. Fully-connected Layer
![DIN.png](https://cdn.jsdelivr.net/gh/lostdesire/lostdesire.github.io/_posts/image/DIN.png)

## BST(Behavior Sequence Transformer)
- CTR 예측과 NLP 번역 데이터 간의 공통점
	- <font color="#ffff00">대부분 sparse feature</font>
	- low/high-order feature interaction 모두 존재 (<font color="#ffff00">비선형적 관계</font>)
- Transformer의 encoder만 사용
- vs DIN
	- local activation layer &rarr; <font color="#ffff00">transformer layer</font>
	- user behavior feature &rarr; <font color="#ffff00">user behavior sequence</font>
- vs Transformer
	- <font color="#ffff00">dropout, leakyReLU</font> 추가
	- <font color="#ffff00">layer 1~4개만</font> 사용 (best는 1개)
	- <font color="#ffff00">Custom Positional Encoding</font>
		- $$pos(v_{i}) = t(v_{t}) - t(v_{i})$$
![BST.png](https://cdn.jsdelivr.net/gh/lostdesire/lostdesire.github.io/_posts/image/BST.png)

## 피어세션
- CF 리뷰
