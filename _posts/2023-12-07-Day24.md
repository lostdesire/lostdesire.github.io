---
title: Day24
date: 2023-12-07 20:00:00 +09:00
categories: [BoostCamp, Week5]
tags: [dl, data, recsys]     # TAG names should always be lowercase
toc: true
toc_sticky: true
use_math: true
---

## MAB(Multi-Armed Bandit)
- MAB의 정책
	- 모든 슬롯머신을 동일한 횟수로 당김
		- 높은 reward 불가능
	- 일정 횟수 후 제일 높은 확률의 슬롯머신만 당김
		- 동일한 슬롯만 계속 당김
- Exploration
	- 더 많은 정보를 얻기 위해 새로운 arm을 선택하는 것
- Exploitation
	- 경험 / 관측을 토대로 가장 좋은 arm을 선택하는 것
- $$q_{*}(a) \doteq \mathbb{E}[R_{t} | A_{t} = a]$$
	- $A_{t}$ : action
	- $R_{t}$ : reward
	- $q_{*}(a)$ : 액션 a에 따른 reward의 실제 기대값
- $q_{*}(a)$에 대한 시간 t에서의 추정치 $Q_{t}(a)$를 최대한 정밀하게 구하는 것이 목표
- greedy action 선택 &rarr; exploitation
- 다른 action 선택 &rarr; exploration
- ### Greedy Algorithm
	- Simple Average Method
	- $$Q_{t}(a) \doteq \frac{sum\ of\ rewards\ when\ a\ taken\ prior\ to\ t}{number\ of\ times\ a\ taken\ prior\ to\ t} = \frac{\sum_{i=1}^{t-1}R_{i}\cdot 1_{A_{i}=a}}{\sum_{i=1}^{t-1}1_{A_{i}=a}}$$
	- 실제 기대값 $q_{*}(a)$의 가장 간단한 추정 방식으로 <font color="#ffff00">표본 평균</font>을 사용
	- 평균 리워드가 최대인 action을 선택하는 것 &rarr; Greedy
		- 처음 선택되는 action과 reward에 크게 영향을 받음(exploration이 부족)
- ### Epsilon-Greedy Algorithm
	- exploration이 부족한 greedy algorithm을 수정
	- ​<font color="#ffff00">일정한 확률</font>에 의해 <font color="#ffff00">랜덤</font>으로 슬롯머신을 선택
	- $$A_{t} \doteq \underset{a}{argmax}\begin{bmatrix}Q_{t}(a) + c\sqrt\frac{\ln t}{N_{t}(a)}\end{bmatrix}$$
		- $Q_{t}(a)$ : action a에 대한 reward의 추정치 (simple average)
		- $N_{t}(a)$ : action a를 선택한 횟수
		- c : exploration을 조정하는 하이퍼파라미터
- ​<font color="#ffff00">개별 아이템</font> &larr; 개별 action
- ​<font color="#ffff00">추천 방식</font> &larr; MAB policy
- ​<font color="#ffff00">클릭 여부</font> &larr; reward
- ### 유저 추천
	- ​<font color="#ffff00">클러스터링</font>을 통해 비슷한 <font color="#ffff00">유저</font>끼리 <font color="#ffff00">그룹화</font>
	- 필요 Bandit 개수 = 유저 클러스터 개수 x 후보 아이템 개수
- ### 유사 아이템 추천
	- 주어진 아이템과 <font color="#ffff00">유사한 후보 아이템 리스트를 생성</font>
	- 필요 Bandit 개수 = 아이템 개수 x 후보 아이템 개수
- ### Thompson Sampling
	- 주어진<font color="#ffff00"> k개의 action에 해당하는 확률분포</font>를 구하는 문제
	- 베타 분포
		- 두 개의 양의 변수로 표현 가능한 확률 분포
		- $$Beta(x|\alpha, \beta) = \frac{1}{B(\alpha, \beta)}x^{\alpha-1}(1-x)^{\beta-1}$$
		- $B(\alpha, \beta)$는 $\alpha, \beta$에 의해 정해지는 베타 함수
		- 배너를 보고 클릭한 횟수 $\alpha$
		- 배너를 보고 클릭하지 않은 횟수 $\beta$
		- 배너를 클릭할 확률 ~ $Beta(\alpha+1, \beta+1)$
		- 초기에는 <font color="#ffff00">랜덤 노출</font> &rarr; <font color="#ffff00">베타 분포에서 샘플링하여 노출</font>
- ### LinUCB
	- Contextual Bandit
	- 유저 context 정보에 따라 동일한 action이더라도 다른 reward를 가짐 (개인화 추천)
	- $$A_{t} \doteq \underset{a}{argmax}\begin{bmatrix}x_{t,a}^{T}\theta_{a}^{*} + \alpha\sqrt{x_{t,a}^{T}A_{a}^{-1}X_{t,a}}\end{bmatrix} \qquad where A_{a} = D_{a}^{T}D_{a}+I_{d}$$
		- $x_{t,a}$ : d-차원 컨텍스트 벡터
		- $\theta_{a}^{*}$ : action a에 대한 d-차원 학습 파라미터
		- $D_{a}$ : m개의 컨텍스트 벡터로 구성된 $m\times d$ 행렬

## 피어세션
- Item2Vec & ANN 리뷰

## 마스터클래스
- 이준원 마스터님의 <**ML Career 만들기 (Feat. AD-Tech)**>
- 이직 : 필수 조건 / 꼭 만족하지 않아도 되는 조건 고려
- 중요한 것 : 1. 개발 능력, 2. ML/DL 지식 및 이해도, 3) 커뮤니케이션 &rarr; <font color="#ffff00">문제해결력</font>
- 주니어 때는 하나의 도메인과 직무에서 깊이를 쌓는 것이 좋다.